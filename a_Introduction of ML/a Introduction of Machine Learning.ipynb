{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3a98b0",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# Introduction of Machine Learning\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd955d",
   "metadata": {},
   "source": [
    "## 1. Supervised & Unsupervised Learning\n",
    "\n",
    "监督学习和非监督学习是机器学习中两种不同的学习范式，它们的主要区别在于训练数据的标签信息是否可用。\n",
    "\n",
    "### 1.1 监督学习（Supervised Learning）：\n",
    "\n",
    "定义：在监督学习中，模型接收带有标签（已知输出）的训练数据。标签是与输入数据相对应的输出，目标是使模型能够学习输入与输出之间的映射关系，从而能够对新的未标记数据进行预测。\n",
    "\n",
    "示例：分类和回归问题是监督学习的经典例子。例如，给定包含猫和狗图像的数据集，并标记每张图像的类别（猫或狗），模型通过学习这些标签来进行分类预测。\n",
    "\n",
    "应用：监督学习常用于需要预测或分类目标的情况，其中有一组已知的输出。\n",
    "\n",
    "### 1.2 非监督学习（Unsupervised Learning）：\n",
    "\n",
    "定义：在非监督学习中，模型接收的训练数据没有标签。模型的任务是发现数据中的结构、模式或规律，而不是根据标签进行预测。非监督学习旨在对数据进行自动学习，发现数据中的隐藏关系。\n",
    "\n",
    "示例：聚类是非监督学习的一个示例。给定一组数据，模型可以自动发现其中的群组，而无需预先知道这些群组的标签。\n",
    "\n",
    "应用：非监督学习常用于数据探索、模式识别、降维等任务，其中我们想要了解数据的内在结构而不依赖于预定义的输出标签。\n",
    "\n",
    "在实践中，还有一种混合型的学习方式，被称为半监督学习（Semi-Supervised Learning），它同时使用带标签和不带标签的数据进行训练。这种方法可以在标注数据有限的情况下充分利用未标注数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8461fa",
   "metadata": {},
   "source": [
    "## 2. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a98d6d",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"a_Regression.png\" alt=\"Image\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "$$\\hat{y}^{(i)}=f_{w,b}({x}^{(i)})$$\n",
    "\n",
    "$$f_{w,b}({x^{(i)}})=w{x^{(i)}}+b$$\n",
    "\n",
    "$$Error = (\\hat{y}^{(i)} - {y}^{(i)})^2 = \\left(f_{w,b}\\left(x^{(i)}\\right)-y^{(i)}\\right)^2$$\n",
    "\n",
    "### 2.1 Objective\n",
    "\n",
    "Find the best parameters w, b that $\\hat{y}^{(i)}$ is close to ${y}^{(i)}$ for all (${x}^{(i)}, {y}^{(i)}$).\n",
    "\n",
    "Well, cost function can help us find best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88731f5a",
   "metadata": {},
   "source": [
    "## 3. Cost function (Squred error cost funtion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303485a",
   "metadata": {},
   "source": [
    "$$J(w,b)=\\frac{1}{2m}\\sum_{i=1}^{m}\\bigl(f_{w,b}\\bigl(x^{(i)}\\bigr)-y^{(i)}\\bigr)^{2}$$\n",
    "\n",
    "$$\\text{m = number of training examples}$$\n",
    "\n",
    "$$\\text{Best parameters w, b can minimize }J(w,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23665714",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"a_bowl.png\" alt=\"Image\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf6858a",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent Algorithm\n",
    "\n",
    "### 4.1 Algorithm\n",
    "\n",
    "In lecture, *gradient descent* was described as:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "### 为何梯度下降每次更新都为$-\\alpha \\frac{\\partial J(w,b)}{\\partial w}$,而不是其他的?\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"a_gradient.jpg\" alt=\"Image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "\n",
    "#### Figure 1\n",
    "\n",
    "</center>\n",
    "\n",
    "1. $\\frac{\\partial J(w,b)}{\\partial w}$会控制下降的方向，如图，当$\\frac{\\partial J(w,b)}{\\partial w}<0$，则$- \\alpha \\frac{\\partial J(w,b)}{\\partial w} > 0$。由于$w = w - \\alpha \\frac{\\partial J(w,b)}{\\partial w}$, 更新后的w所在点的J(w)的值会更加接近最小值。这也就是所谓的w会朝梯度下降的方向更新。\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"a_alpha.jpg\" alt=\"Image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "\n",
    "#### Figure 2\n",
    "\n",
    "</center>\n",
    "\n",
    "2. 实际的梯度下降步伐并没有Figure 1那么大, 如Figure 2，因为我们取的$\\alpha$值都比较小。\n",
    "\n",
    "3. 此外，在梯度下降的过程中这个偏导数的绝对值是趋近于0的，会越来越小，直到不变。而与此同时，w的更新也就越来越小，直到不变，成为最优w。\n",
    "\n",
    "\n",
    "### 4.2. Parameters Updated simultaneously\n",
    "\n",
    "Parameters $w$, $b$ are updated simultaneously.  \n",
    "\n",
    "\n",
    "#### 4.2.1 The gradient \n",
    "\n",
    "The gradient is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{1} \\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters. In the code, they are always shown as that:\n",
    "\n",
    "__Only if firstly updating all formula (1) and (2), then you can update w and b in formula (3) and (4).__\n",
    "\n",
    "$$ w = w -\\alpha\\frac{\\partial}{\\partial w}J(w,b) \\tag{3} $$\n",
    "\n",
    "$$ b = b -\\alpha\\frac{\\partial}{\\partial b}J(w,b) \\tag{4} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf48f3",
   "metadata": {},
   "source": [
    "### 4.3 Local Minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c9f46",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"a_localMinimal.png\" alt=\"Image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "不同start point, 可能会到达Cost function J的不同Local Minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa36b254",
   "metadata": {},
   "source": [
    "### 4.4 Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a119cdc",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"a_batch.png\" alt=\"Image\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8dbf4",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# Summary of the notation\n",
    "\n",
    "</center>\n",
    " \n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{x}$ | Training Example feature values (in this lab - Size (1000 sqft))  | `x_train` |   \n",
    "|  $\\mathbf{y}$  | Training Example  targets (in this lab Price (1000s of dollars)).  | `y_train` \n",
    "|  $x^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `x_i`, `y_i`|\n",
    "| m | Number of training examples | `m`|\n",
    "|  $w$  |  parameter: weight,                                 | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |\n",
    "| $\\hat{y}$|$\\hat{y}$ is prediction value, for example：$\\hat{y} = w_0 + w_1x_1 + ... + w_{p}x_{p}$||\n",
    "| $f_{w,b}(x^{(i)})$ | The result of the model evaluation at $x^{(i)}$ parameterized by $w,b$: $f_{w,b}(x^{(i)}) = wx^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff6ba7",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# REFERENCE\n",
    "\n",
    "</center>\n",
    "\n",
    "[1]https://github.com/kaieye/2022-Machine-Learning-Specialization/blob/main/Supervised%20Machine%20Learning%20Regression%20and%20Classification/week1/6.Train%20the%20model%20with%20gradient%20descent/C1_W1_Lab05_Gradient_Descent_Soln.ipynb\n",
    "\n",
    "[2]https://www.coursera.org/specializations/machine-learning-introduction\n",
    "\n",
    "[3]https://www.bilibili.com/video/BV1Pa411X76s?p=20&spm_id_from=pageDriver&vd_source=8c32dd2bfbfecb1eaa9b0b9c4fb4d83e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
